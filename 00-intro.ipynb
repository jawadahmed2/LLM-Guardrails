{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OWG3JPB0jUJo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    nemoguardrails==0.4.0 \\\n",
        "    openai==0.27.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDrsyThjjUJp"
      },
      "source": [
        "We need to set our OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oHQOTiTQjUJp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\") or \"sk-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcm8FnQ_jUJp"
      },
      "source": [
        "We can run Guardrails from terminal using the command:\n",
        "\n",
        "```\n",
        "nemoguardrails chat --config=config/\n",
        "```\n",
        "\n",
        "Where the `config` directory must contain our `config.yml` and a Colang file (like `topics.co`).\n",
        "\n",
        "Alternatively we can load them from file using `RailsConfig.from_path(\"./config\")` or from string variables in our code like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DhZ9qy90jUJq"
      },
      "outputs": [],
      "source": [
        "yaml_content = \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: openai\n",
        "  model: text-davinci-003\n",
        "\"\"\"\n",
        "colang_content = \"\"\"\n",
        "# define niceties\n",
        "define user express greeting\n",
        "    \"hello\"\n",
        "    \"hi\"\n",
        "    \"what's up?\"\n",
        "\n",
        "define flow greeting\n",
        "    user express greeting\n",
        "    bot express greeting\n",
        "    bot ask how are you\n",
        "\n",
        "# define limits\n",
        "define user ask politics\n",
        "    \"what are your political beliefs?\"\n",
        "    \"thoughts on the president?\"\n",
        "    \"left wing\"\n",
        "    \"right wing\"\n",
        "\n",
        "define bot answer politics\n",
        "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
        "\n",
        "define flow politics\n",
        "    user ask politics\n",
        "    bot answer politics\n",
        "    bot offer help\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PrKzVn4wjUJq"
      },
      "outputs": [],
      "source": [
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "# initialize rails config\n",
        "config = RailsConfig.from_content(\n",
        "  \tyaml_content=yaml_content,\n",
        "    colang_content=colang_content\n",
        ")\n",
        "# create rails\n",
        "rails = LLMRails(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdI13hg4jUJq"
      },
      "source": [
        "From here, we begin asking questions and interacting with our Guardrails protected LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htzi4K72jUJq",
        "outputId": "8615c158-9d06-4423-b454-d7489ddb9e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! How can I help you?\n",
            "How are you doing today?\n"
          ]
        }
      ],
      "source": [
        "res = await rails.generate_async(prompt=\"Hey there!\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2qv5mEujUJq"
      },
      "source": [
        "This is a typical greeting so we have no protective guardrails here. However, we do see the `greeting` flow being activated as the chatbot generates some text from the `bot express greeting` message, and on the next line generates some more text from the `bot ask how are you` message.\n",
        "\n",
        "Let's try asking a more political question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-e24M6PjUJq",
        "outputId": "e8f5c8b4-4a1a-495f-f6a9-4d86864933c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm a shopping assistant, I don't like to talk of politics.\n",
            "However, I can help you with shopping related tasks. Is there anything I can help you with?\n"
          ]
        }
      ],
      "source": [
        "res = await rails.generate_async(prompt=\"what do you think of the president?\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IizgBc-jUJr"
      },
      "source": [
        "Here we can see that our `politics` rail is activated and our chatbot dodges the question with `bot answer politics` and follows up with `bot offer help`.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}